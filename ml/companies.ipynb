{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching company data: Invalid URL '/httpservice/retry/enablejs?sei=78qdZ-fuOu7R1e8Pg5SB6AU': No scheme supplied. Perhaps you meant https:///httpservice/retry/enablejs?sei=78qdZ-fuOu7R1e8Pg5SB6AU?\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Dict\n",
    "from groq import Groq\n",
    "\n",
    "def get_company_info(company_name: str, api_key: str) -> Optional[Dict[str, str]]:\n",
    "    try:\n",
    "        # Web scraping part remains the same\n",
    "        search_url = f\"https://www.google.com/search?q={company_name.replace(' ', '+')}+official+website?\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        search_response = requests.get(search_url, headers=headers)\n",
    "        search_soup = BeautifulSoup(search_response.text, \"html.parser\")\n",
    "        \n",
    "        first_link = search_soup.find(\"a\")\n",
    "        if not first_link:\n",
    "            return None\n",
    "        \n",
    "        company_url = first_link.get(\"href\")\n",
    "        response = requests.get(company_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        name = soup.find(\"title\").text.strip() if soup.find(\"title\") else \"Unknown\"\n",
    "        description_meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = description_meta[\"content\"].strip() if description_meta else \"No description available\"\n",
    "        \n",
    "        # Initialize Groq client\n",
    "        client = Groq(api_key=api_key)\n",
    "        \n",
    "        # Create the prompt for analysis\n",
    "        prompt = f\"Extract the company name, description, and location from the following text. Format the response as a clear, concise summary: {description}\"\n",
    "        \n",
    "        # Make the API call to Groq\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant that extracts and summarizes company information.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama2-70b-4096\",  # You can also use \"llama2-70b-4096\"\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Extract the analysis from Groq's response\n",
    "        ai_analysis = completion.choices[0].message.content\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"description\": ai_analysis,\n",
    "            \"location\": company_url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching company data:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Group Surat\"\n",
    "    api_key = \"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\"  # Replace with your actual Groq API key\n",
    "    company_info = get_company_info(company_name, api_key)\n",
    "    print(company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Google Search Help', 'website': 'https://support.google.com/websearch', 'description': '1. Company Name: Google Search Help\\n2. Main Business/Industry: Online Search Engine Services\\n3. Location: Information not explicitly provided, but Google is headquartered in Mountain View, California, USA.\\n4. Key Products/Services: Offical Google Search Help Center, providing tips, tutorials, and answers to frequently asked questions about using Google Search.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Dict\n",
    "from groq import Groq\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_company_info(company_name: str, api_key: str) -> Optional[Dict[str, str]]:\n",
    "    try:\n",
    "        # Modified web scraping part with better URL handling\n",
    "        search_url = f\"https://www.google.com/search?q={company_name.replace(' ', '+')}+official+website\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "        \n",
    "        search_response = requests.get(search_url, headers=headers)\n",
    "        search_response.raise_for_status()\n",
    "        search_soup = BeautifulSoup(search_response.text, \"html.parser\")\n",
    "        \n",
    "        # Look specifically for search result links\n",
    "        search_results = search_soup.select(\".yuRUbf a\")  # Google's search result class\n",
    "        if not search_results:\n",
    "            search_results = search_soup.select(\"div.g div.r a\")  # Alternative class names\n",
    "        if not search_results:\n",
    "            search_results = search_soup.find_all(\"a\", href=True)  # Fallback to all links\n",
    "            \n",
    "        # Find first valid URL\n",
    "        company_url = None\n",
    "        for result in search_results:\n",
    "            href = result.get(\"href\", \"\")\n",
    "            if href.startswith((\"http://\", \"https://\")):\n",
    "                # Validate URL\n",
    "                parsed = urlparse(href)\n",
    "                if parsed.scheme and parsed.netloc:\n",
    "                    company_url = href\n",
    "                    break\n",
    "        \n",
    "        if not company_url:\n",
    "            raise ValueError(\"No valid company URL found\")\n",
    "            \n",
    "        # Fetch company website\n",
    "        response = requests.get(company_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract title and description with better error handling\n",
    "        name = soup.find(\"title\")\n",
    "        name = name.text.strip() if name else company_name\n",
    "        \n",
    "        description = \"\"\n",
    "        # Try multiple meta tag variations\n",
    "        for meta in soup.find_all(\"meta\"):\n",
    "            if meta.get(\"name\", \"\").lower() == \"description\" or meta.get(\"property\", \"\").lower() == \"og:description\":\n",
    "                description = meta.get(\"content\", \"\").strip()\n",
    "                break\n",
    "        \n",
    "        if not description:\n",
    "            # Fallback to first paragraph or div text\n",
    "            first_p = soup.find(\"p\")\n",
    "            if first_p:\n",
    "                description = first_p.text.strip()\n",
    "            else:\n",
    "                first_div = soup.find(\"div\")\n",
    "                description = first_div.text.strip() if first_div else \"No description available\"\n",
    "        \n",
    "        # Initialize Groq client\n",
    "        client = Groq(api_key=api_key)\n",
    "        \n",
    "        # Create the prompt for analysis\n",
    "        prompt = f\"\"\"Analyze and extract key information about this company:\n",
    "        Name: {name}\n",
    "        Website: {company_url}\n",
    "        Description: {description}\n",
    "        \n",
    "        Please provide a concise summary including:\n",
    "        1. Company name\n",
    "        2. Main business/industry\n",
    "        3. Location (if available)\n",
    "        4. Key products/services\"\"\"\n",
    "        \n",
    "        # Make the API call to Groq\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant that extracts and summarizes company information in a clear, structured format.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Extract the analysis from Groq's response\n",
    "        ai_analysis = completion.choices[0].message.content\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"website\": company_url,\n",
    "            \"description\": ai_analysis\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching company data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Group Surat\"\n",
    "    api_key = \"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\"  # Replace with your actual Groq API key\n",
    "    company_info = get_company_info(company_name, api_key)\n",
    "    print(company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wikipedia in c:\\users\\mjgan\\appdata\\roaming\\python\\python311\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "No Wikipedia page found.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia\n",
    "import wikipedia\n",
    "\n",
    "def get_wikipedia_summary(company_name):\n",
    "    try:\n",
    "        summary = wikipedia.summary(company_name, sentences=2)\n",
    "        return summary\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return f\"Multiple results found: {e.options}\"\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return \"No Wikipedia page found.\"\n",
    "\n",
    "print(get_wikipedia_summary(\"Amazon\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search for: Avadh Projects Surat\n",
      "Searching Google for: https://www.google.com/search?q=Avadh+Projects+Surat+real+estate+developer+Surat&num=10\n",
      "\n",
      "Found 3 links\n",
      "Could not find any relevant websites.\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "from groq import Groq\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "def scrape_google(company_name):\n",
    "    \"\"\"Scrapes first relevant Google Search result.\"\"\"\n",
    "    search_query = {\n",
    "        'q': f\"{company_name} real estate developer Surat\",\n",
    "        'num': '10'  # We only need first few results now\n",
    "    }\n",
    "    search_url = f\"https://www.google.com/search?{urlencode(search_query)}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Searching Google for: {search_url}\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Get all links\n",
    "        all_links = soup.find_all(\"a\", href=True)\n",
    "        print(f\"\\nFound {len(all_links)} links\")\n",
    "        \n",
    "        # Return first valid external link\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if href.startswith(\"http\") and \"google\" not in href:\n",
    "                print(f\"Selected URL: {href}\")\n",
    "                return href\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in scrape_google: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrapes the website for basic details.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nScraping: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Get title\n",
    "        title_tag = soup.find(\"title\")\n",
    "        name = title_tag.text.strip() if title_tag else \"No Name\"\n",
    "        print(f\"Found title: {name}\")\n",
    "        \n",
    "        # Get description\n",
    "        description = \"\"\n",
    "        \n",
    "        # Try meta descriptions first\n",
    "        meta_desc = soup.find(\"meta\", {\"name\": \"description\"}) or \\\n",
    "                   soup.find(\"meta\", {\"property\": \"og:description\"})\n",
    "        if meta_desc and meta_desc.get(\"content\"):\n",
    "            description = meta_desc[\"content\"].strip()\n",
    "        \n",
    "        # If no meta description, get first few paragraphs\n",
    "        if not description:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            description = \" \".join(p.text.strip() for p in paragraphs[:3])\n",
    "        \n",
    "        if not description:\n",
    "            description = \"No description available\"\n",
    "            \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"website\": url,\n",
    "            \"description\": description[:1000]  # Limit description length\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in scrape_website: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def summarize_with_groq(company_info, api_key):\n",
    "    \"\"\"Uses Groq API to summarize company data.\"\"\"\n",
    "    if not company_info:\n",
    "        return \"No company information available to summarize.\"\n",
    "        \n",
    "    try:\n",
    "        client = Groq(api_key=api_key)\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this real estate developer company in Surat:\n",
    "        Name: {company_info['name']}\n",
    "        Website: {company_info['website']}\n",
    "        Description: {company_info['description']}\n",
    "        \n",
    "        Please provide a detailed summary including:\n",
    "        1. Company overview and main business focus\n",
    "        2. Types of projects and developments\n",
    "        3. Notable features or specializations\n",
    "        4. Market presence in Surat\n",
    "        \"\"\"\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a real estate industry analyst focusing on the Surat market. Provide concise but informative analysis.\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarize_with_groq: {str(e)}\")\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Projects Surat\"\n",
    "    groq_api_key = \"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\"\n",
    "    \n",
    "    print(f\"Starting search for: {company_name}\")\n",
    "    website_url = scrape_google(company_name)\n",
    "    \n",
    "    if website_url:\n",
    "        print(f\"\\nFound website: {website_url}\")\n",
    "        company_data = scrape_website(website_url)\n",
    "        if company_data:\n",
    "            print(\"\\nGenerating summary...\")\n",
    "            summary = summarize_with_groq(company_data, groq_api_key)\n",
    "            print(\"\\nCompany Analysis:\")\n",
    "            print(summary)\n",
    "        else:\n",
    "            print(\"Failed to scrape website data\")\n",
    "    else:\n",
    "        print(\"Could not find any relevant websites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 53 (1041811738.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 58\u001b[1;36m\u001b[0m\n\u001b[1;33m    def summarize_with_groq(company_info, api_key):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 53\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from groq import Groq\n",
    "from urllib.parse import urlencode, urlparse\n",
    "\n",
    "def scrape_google(company_name):\n",
    "    \"\"\"Scrapes Google Search results for a company website.\"\"\"\n",
    "    search_query = {\n",
    "        'q': f\"{company_name} real estate developer Surat\",\n",
    "        'num': '10'\n",
    "    }\n",
    "    search_url = f\"https://www.google.com/search?{urlencode(search_query)}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"Searching Google for: {search_url}\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        all_links = soup.find_all(\"a\", href=True)\n",
    "        print(f\"\\nFound {len(all_links)} links\")\n",
    "\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\", \"\")\n",
    "\n",
    "            if href.startswith(\"http\"):\n",
    "                parsed_url = urlparse(href)\n",
    "                hostname = parsed_url.netloc\n",
    "\n",
    "                if not any(domain in hostname for domain in [\"google.com\", \"bing.com\", \"yahoo.com\"]):\n",
    "                    print(f\"Selected URL: {href}\")\n",
    "                    return href\n",
    "\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in scrape_google: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in scrape_google: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrapes the website for basic details.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nScraping: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        title_tag = soup.find(\"title\")\n",
    "        name = title_tag.text.strip() if title_tag else \"No Name\"\n",
    "        print(f\"Found title: {name}\")\n",
    "\n",
    "        description = \"\"\n",
    "\n",
    "        meta_desc = soup.find(\"meta\", {\"name\": \"description\"}) or \\\n",
    "                    soup.find(\"meta\", {\"property\": \"og:description\"})\n",
    "        if meta_desc and meta_desc.get(\"content\"):\n",
    "            description = meta_desc[\"content\"].strip()\n",
    "\n",
    "        if not description:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            description = \" \".join(p.text.strip() for p in paragraphs[:3])\n",
    "\n",
    "        if not description:\n",
    "            description = \"No description available\"\n",
    "\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"website\": url,\n",
    "            \"description\": description[:1000]\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def summarize_with_groq(company_info, api_key):\n",
    "    \"\"\"Uses Groq API to summarize company data.\"\"\"\n",
    "    if not company_info:\n",
    "        return \"No company information available to summarize.\"\n",
    "\n",
    "    try:\n",
    "        client = Groq(api_key=api_key)\n",
    "\n",
    "        prompt = f\"\"\"Analyze this real estate developer company in Surat:\n",
    "        Name: {company_info['name']}\n",
    "        Website: {company_info['website']}\n",
    "        Description: {company_info['description']}\n",
    "\n",
    "        Please provide a detailed summary including:\n",
    "        1. Company overview and main business focus\n",
    "        2. Types of projects and developments\n",
    "        3. Notable features or specializations\n",
    "        4. Market presence in Surat\n",
    "        \"\"\"\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a real estate industry analyst focusing on the Surat market. Provide concise but informative analysis.\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"mixtral-8x7b-32768\",  # Or another suitable model\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarize_with_groq: {str(e)}\")\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Projects Surat\"  # Example\n",
    "    groq_api_key = \"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\"  # Replace with your actual key\n",
    "\n",
    "    print(f\"Starting search for: {company_name}\")\n",
    "    website_url = scrape_google(company_name)\n",
    "\n",
    "    if website_url:\n",
    "        print(f\"\\nFound website: {website_url}\")\n",
    "        company_data = scrape_website(website_url)\n",
    "        if company_data:\n",
    "            print(\"\\nGenerating summary...\")\n",
    "            summary = summarize_with_groq(company_data, groq_api_key)\n",
    "            print(\"\\nCompany Analysis:\")\n",
    "            print(summary)\n",
    "        else:\n",
    "            print(\"Failed to scrape website data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 14:07:50,991 - ERROR - GROQ_API_KEY environment variable not set.\n",
      "2025-02-01 14:07:50,993 - INFO - Starting search for: Avadh Projects Surat\n",
      "2025-02-01 14:07:50,995 - INFO - Searching Google for: https://www.google.com/search?q=Avadh+Projects+Surat+real+estate+developer+Surat&num=10\n",
      "2025-02-01 14:07:52,855 - INFO - Found 3 links\n",
      "2025-02-01 14:07:52,868 - ERROR - Could not find any relevant websites.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from groq import Groq\n",
    "from urllib.parse import urlencode, urlparse\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def scrape_google(company_name):\n",
    "    \"\"\"Scrapes Google Search results for a company website.\"\"\"\n",
    "    search_query = {\n",
    "        'q': f\"{company_name} real estate developer Surat\",\n",
    "        'num': '10'  # Adjust the number of results\n",
    "    }\n",
    "    search_url = f\"https://www.google.com/search?{urlencode(search_query)}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Searching Google for: {search_url}\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\", href=True)\n",
    "        logging.info(f\"Found {len(all_links)} links\")\n",
    "\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if href.startswith(\"http\"):\n",
    "                parsed_url = urlparse(href)\n",
    "                hostname = parsed_url.netloc\n",
    "\n",
    "                # Filter out search engine domains and check for company name in URL\n",
    "                if (not any(domain in hostname for domain in [\"google.com\", \"bing.com\", \"yahoo.com\"]) and\n",
    "                    company_name.lower() in hostname.lower()):\n",
    "                    logging.info(f\"Selected URL: {href}\")\n",
    "                    return href  # Return the first valid link\n",
    "\n",
    "        return None  # No valid link found\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error in scrape_google: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in scrape_google: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrape company website for relevant information.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Scraping website: {url}\")\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Extract relevant data (e.g., company name, description, projects)\n",
    "        company_name = soup.title.string if soup.title else \"Unknown\"\n",
    "        description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = description[\"content\"] if description else \"No description available.\"\n",
    "\n",
    "        return {\n",
    "            \"name\": company_name,\n",
    "            \"website\": url,\n",
    "            \"description\": description,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error in scrape_website: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in scrape_website: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summarize_with_groq(company_info, api_key):\n",
    "    \"\"\"Summarize company information using Groq API.\"\"\"\n",
    "    if not company_info:\n",
    "        return \"No company information available to summarize.\"\n",
    "\n",
    "    client = Groq(api_key=api_key)\n",
    "\n",
    "    prompt = f\"\"\"Analyze this real estate developer company in Surat:\n",
    "    Name: {company_info['name']}\n",
    "    Website: {company_info['website']}\n",
    "    Description: {company_info['description']}\n",
    "\n",
    "    Please provide a detailed summary including:\n",
    "    1. Company overview and main business focus\n",
    "    2. Types of projects and developments\n",
    "    3. Notable features or specializations\n",
    "    4. Market presence in Surat\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a real estate industry analyst focusing on the Surat market. Provide concise but informative analysis.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",  # Or another suitable model\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Projects Surat\"  # Replace with the desired company name\n",
    "    groq_api_key = os.getenv(\"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\")  # Use environment variable for API key\n",
    "\n",
    "    if not groq_api_key:\n",
    "        logging.error(\"GROQ_API_KEY environment variable not set.\")\n",
    "        exit(1)\n",
    "\n",
    "    logging.info(f\"Starting search for: {company_name}\")\n",
    "    website_url = scrape_google(company_name)\n",
    "\n",
    "    if website_url:\n",
    "        logging.info(f\"Found website: {website_url}\")\n",
    "        company_data = scrape_website(website_url)\n",
    "\n",
    "        if company_data:\n",
    "            logging.info(\"Generating summary...\")\n",
    "            summary = summarize_with_groq(company_data, groq_api_key)\n",
    "            logging.info(\"\\nCompany Analysis:\")\n",
    "            print(summary)\n",
    "        else:\n",
    "            logging.error(\"Failed to scrape website data\")\n",
    "    else:\n",
    "        logging.error(\"Could not find any relevant websites.\")\n",
    "\n",
    "    time.sleep(2)  # Delay for the next search (if doing multiple searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 14:10:53,741 - ERROR - GROQ_API_KEY environment variable not set.\n",
      "2025-02-01 14:10:53,743 - INFO - Starting search for: Avadh Projects Surat\n",
      "2025-02-01 14:10:53,745 - INFO - Searching Bing for: https://www.bing.com/search?q=Avadh+Projects+Surat+real+estate+developer+Surat&count=10\n",
      "2025-02-01 14:10:54,963 - INFO - Found 104 links\n",
      "2025-02-01 14:10:54,969 - ERROR - Could not find any relevant websites.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from groq import Groq\n",
    "from urllib.parse import urlencode, urlparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def scrape_bing(company_name):\n",
    "    \"\"\"Scrapes Bing Search results to find the first relevant company website.\"\"\"\n",
    "    search_query = {\n",
    "        'q': f\"{company_name} real estate developer Surat\",\n",
    "        'count': '10'  # Number of results to retrieve\n",
    "    }\n",
    "    search_url = f\"https://www.bing.com/search?{urlencode(search_query)}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://www.bing.com/\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Searching Bing for: {search_url}\")\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\", href=True)\n",
    "        logging.info(f\"Found {len(all_links)} links\")\n",
    "\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if href.startswith(\"http\"):\n",
    "                parsed_url = urlparse(href)\n",
    "                hostname = parsed_url.netloc\n",
    "\n",
    "                # Filter out Bing-related domains and check for company name in URL\n",
    "                if (not any(domain in hostname for domain in [\"bing.com\", \"microsoft.com\"]) and\n",
    "                    company_name.lower() in hostname.lower()):\n",
    "                    logging.info(f\"Selected URL: {href}\")\n",
    "                    return href  # Return the first valid link\n",
    "\n",
    "        return None  # No valid link found\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error in scrape_bing: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in scrape_bing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrape the first website for basic information.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Scraping website: {url}\")\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Extract basic information\n",
    "        company_name = soup.title.string if soup.title else \"Unknown\"\n",
    "        description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = description[\"content\"] if description else \"No description available.\"\n",
    "\n",
    "        return {\n",
    "            \"name\": company_name,\n",
    "            \"website\": url,\n",
    "            \"description\": description,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error in scrape_website: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error in scrape_website: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summarize_with_groq(company_info, api_key):\n",
    "    \"\"\"Summarize company information using Groq API.\"\"\"\n",
    "    if not company_info:\n",
    "        return \"No company information available to summarize.\"\n",
    "\n",
    "    client = Groq(api_key=api_key)\n",
    "\n",
    "    prompt = f\"\"\"Analyze this real estate developer company in Surat:\n",
    "    Name: {company_info['name']}\n",
    "    Website: {company_info['website']}\n",
    "    Description: {company_info['description']}\n",
    "\n",
    "    Please provide a brief summary including:\n",
    "    1. Company overview\n",
    "    2. Main business focus\n",
    "    3. Notable features or specializations\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a real estate industry analyst focusing on the Surat market. Provide concise but informative analysis.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",  # Or another suitable model\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Projects Surat\"  # Replace with the desired company name\n",
    "    groq_api_key = os.getenv(\"GROQ_API_KEY\")  # Use environment variable for API key\n",
    "\n",
    "    if not groq_api_key:\n",
    "        logging.error(\"GROQ_API_KEY environment variable not set.\")\n",
    "        exit(1)\n",
    "\n",
    "    logging.info(f\"Starting search for: {company_name}\")\n",
    "    website_url = scrape_bing(company_name)\n",
    "\n",
    "    if website_url:\n",
    "        logging.info(f\"Found website: {website_url}\")\n",
    "        company_data = scrape_website(website_url)\n",
    "\n",
    "        if company_data:\n",
    "            logging.info(\"Generating summary...\")\n",
    "            summary = summarize_with_groq(company_data, groq_api_key)\n",
    "            logging.info(\"\\nCompany Analysis:\")\n",
    "            print(summary)\n",
    "        else:\n",
    "            logging.error(\"Failed to scrape website data\")\n",
    "    else:\n",
    "        logging.error(\"Could not find any relevant websites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the official website.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def bing_search(company_name):\n",
    "    \"\"\"Scrapes Bing Search to find the official website.\"\"\"\n",
    "    search_url = f\"https://www.bing.com/search?q={company_name.replace(' ', '+')}+real+estate+Surat\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract the first valid website link\n",
    "    for link in soup.select(\"li.b_algo a\"):\n",
    "        href = link[\"href\"]\n",
    "        if \"avadhprojects.com\" in href:  # Official website check\n",
    "            return href\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_company_website(url):\n",
    "    \"\"\"Scrapes the company's official website for details.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    name = soup.find(\"title\").text.strip() if soup.find(\"title\") else \"No Name\"\n",
    "    \n",
    "    description = \"\"\n",
    "    meta_desc = soup.find(\"meta\", {\"name\": \"description\"})\n",
    "    if meta_desc:\n",
    "        description = meta_desc[\"content\"]\n",
    "    else:\n",
    "        first_p = soup.find(\"p\")\n",
    "        description = first_p.text.strip() if first_p else \"No description available\"\n",
    "\n",
    "    return {\"name\": name, \"website\": url, \"description\": description}\n",
    "\n",
    "def scrape_competitors():\n",
    "    \"\"\"Scrapes real estate competitors from a listing website.\"\"\"\n",
    "    competitors_url = \"https://houssed.com/blog/guides/top-builders-in-surat\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(competitors_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    competitors = []\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        name = h3.text.strip()\n",
    "        competitors.append(name)\n",
    "\n",
    "    return competitors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Group Surat\"\n",
    "\n",
    "    website_url = bing_search(company_name)\n",
    "    \n",
    "    if website_url:\n",
    "        company_data = scrape_company_website(website_url)\n",
    "        competitors = scrape_competitors()\n",
    "        \n",
    "        print(\"üè¢ **Company Info:**\")\n",
    "        print(company_data)\n",
    "        print(\"\\nüèóÔ∏è **Top Competitors:**\")\n",
    "        print(\"\\n\".join(competitors))\n",
    "    else:\n",
    "        print(\"Could not find the official website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 15:21:41,149 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¢ Company Information:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"company\": {\n",
      "        \"official_name\": \"Reliance Industries Limited\",\n",
      "        \"industry_type\": \"Conglomerate\",\n",
      "        \"headquarters\": \"Mumbai, India\",\n",
      "        \"key_products_services\": [\n",
      "            \"Petroleum refining and marketing\",\n",
      "            \"Petrochemicals\",\n",
      "            \"Retail\",\n",
      "            \"Digital services\",\n",
      "            \"Telecommunications\"\n",
      "        ],\n",
      "        \"website\": \"https://www.ril.com/\"\n",
      "    },\n",
      "    \"competitors\": [\n",
      "        {\n",
      "            \"name\": \"Tata Group\",\n",
      "            \"industry_type\": \"Conglomerate\",\n",
      "            \"headquarters\": \"Mumbai, India\",\n",
      "            \"key_products_services\": [\n",
      "                \"Automotive manufacturing\",\n",
      "                \"Steel production\",\n",
      "                \"Power generation\",\n",
      "                \"Telecommunications\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Adani Group\",\n",
      "            \"industry_type\": \"Conglomerate\",\n",
      "            \"headquarters\": \"Ahmedabad, India\",\n",
      "            \"key_products_services\": [\n",
      "                \"Commodities trading\",\n",
      "                \"Port operations\",\n",
      "                \"Power generation\",\n",
      "                \"Renewable energy\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Hinduja Group\",\n",
      "            \"industry_type\": \"Conglomerate\",\n",
      "            \"headquarters\": \"Mumbai, India\",\n",
      "            \"key_products_services\": [\n",
      "                \"Automotive manufacturing\",\n",
      "                \"Banking and finance\",\n",
      "                \"Cable television\",\n",
      "                \"Real estate development\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Birla Group\",\n",
      "            \"industry_type\": \"Conglomerate\",\n",
      "            \"headquarters\": \"Mumbai, India\",\n",
      "            \"key_products_services\": [\n",
      "                \"Cement production\",\n",
      "                \"Chemicals manufacturing\",\n",
      "                \"Fertilizers production\",\n",
      "                \"Telecommunications\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Essar Group\",\n",
      "            \"industry_type\": \"Conglomerate\",\n",
      "            \"headquarters\": \"Mumbai, India\",\n",
      "            \"key_products_services\": [\n",
      "                \"Steel production\",\n",
      "                \"Oil refining\",\n",
      "                \"Power generation\",\n",
      "                \"Telecommunications\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "\n",
    "def get_company_info(company_name: str, location: str, api_key: str):\n",
    "    \"\"\"Fetches company details using Groq AI.\"\"\"\n",
    "    client = Groq(api_key=api_key)\n",
    "\n",
    "    # Create a structured prompt for the AI model\n",
    "    prompt = f\"\"\"\n",
    "    Extract key details about the company based on the given prompt:\n",
    "    \n",
    "    Company: {company_name}\n",
    "    Location: {location}\n",
    "    \n",
    "    Provide the following details:\n",
    "    1. Official Name\n",
    "    2. Industry Type\n",
    "    3. Headquarters / Main Office Location\n",
    "    4. Key Products / Services\n",
    "    5. Website (if available)\n",
    "\n",
    "    Write the competitiors based on the same location, sector and services\n",
    "\n",
    "    Strictly always return response in JSON format as shown in example\n",
    "    \"\"\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "    Example: \n",
    "    Input :\n",
    "    {\n",
    "        company_name = \"Avadh Group\",\n",
    "        location = \"Surat, India\"\n",
    "    }\n",
    "\n",
    "    Output : \n",
    "    {\n",
    "    \"company\": {\n",
    "        \"official_name\": \"Avadh Group\",\n",
    "        \"industry_type\": \"Real Estate and Construction\",\n",
    "        \"headquarters\": \"Surat, India\",\n",
    "        \"key_products_services\": [\n",
    "        \"Development of residential and commercial projects (apartments, villas, office spaces)\",\n",
    "        \"Property management\",\n",
    "        \"Interior design\",\n",
    "        \"Construction\"\n",
    "        ],\n",
    "        \"website\": \"Not Available\"\n",
    "    },\n",
    "    \"competitors\": [\n",
    "        {\n",
    "        \"name\": \"Sangini Group\",\n",
    "        \"industry_type\": \"Real Estate and Construction\",\n",
    "        \"headquarters\": \"Surat, India\",\n",
    "        \"key_products_services\": [\n",
    "            \"Residential and commercial real estate development\"\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"Rajhans Group\",\n",
    "        \"industry_type\": \"Real Estate and Construction\",\n",
    "        \"headquarters\": \"Surat, India\",\n",
    "        \"key_products_services\": [\n",
    "            \"Property development\",\n",
    "            \"Interior design\",\n",
    "            \"Construction\"\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"JRD Group\",\n",
    "        \"industry_type\": \"Real Estate and Construction\",\n",
    "        \"headquarters\": \"Surat, India\",\n",
    "        \"key_products_services\": [\n",
    "            \"Development of residential and commercial projects (apartments, villas, office spaces)\"\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"BK Jewels\",\n",
    "        \"industry_type\": \"Real Estate and Construction\",\n",
    "        \"headquarters\": \"Surat, India\",\n",
    "        \"key_products_services\": [\n",
    "            \"Residential and commercial real estate development (apartments, villas, office spaces)\"\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"SPC Group\",\n",
    "        \"industry_type\": \"Real Estate and Construction\",\n",
    "        \"headquarters\": \"Surat, India\",\n",
    "        \"key_products_services\": [\n",
    "            \"Property development\",\n",
    "            \"Interior design\",\n",
    "            \"Construction\"\n",
    "        ]\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Call Groq API\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts structured company information.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # Extract response\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = \"Avadh Group\"\n",
    "    location = \"Surat, India\"\n",
    "    api_key = \"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\"  # Replace with your actual Groq API key\n",
    "\n",
    "    company_info = get_company_info(company_name, location, api_key)\n",
    "    print(\"üè¢ Company Information:\\n\")\n",
    "    print(company_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
